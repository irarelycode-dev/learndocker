docker pull
docker run <image>
docker run -p host_port:container_port <image>
docker run -d -p host_port:container_port <image>
docker run --detach -p host_port:container_port <image>
docker stop <container_id/container_name>
docker run -d -p host_port:container_port --name <container_name> <image>
docker ps or docker container ls
docker ps -a or docker container ls -a
docker logs <container_name>
docker top <container_name> //displays the running process of the container

what happens when in "docker run"
1. Looks for image locally in the cache
2. If not, looks in the remote image repository (defaults to Docker Hub)
3. download the latest version (nginx:latest by default)
4. Creates a new container based on that image and prepares to start
5. Gives it a virtual IP on a private network inside a docker engine
6. opens up a port and runs

docker image rm <image_name>
docker exec -it <image_name> sh
docker start <container_id>
docker kill <container_id>
docker stop <container_id>
docker system prune //removes all containers, all networks not used by atleast one container, all dangling images and build cache
docker images
docker container rm <container_name>
docker container inspeact <container_id> //returns a json of how the container was started.
docker container stats //returns live status of the running containers
docker container run -it //starts new container interactively
docker container exec -it //run additional command in existing container

docker exec -it ubuntu sh
# apt-get install -y curl
#curl google.com
#exit 

docker exec -it mariadb bash //this runs an additional process on an existing running container
#ps aux  //it is used to monitor processes running on a linux system



docker networks
1. each container is connected to a private virtual network called "bridge"
2.Each virtual network routes through NAT firewall on host IP
3.All containers on a virtual network can talk to each other without -p
4.Best practice is to create a new virtual network for each app


docker network ls
docker network inspect <net_name> //summary of the network. All containers running on that network.
docker run -d --name new_nginx --network my_app_net nginx
docker network create my_app_net
docker network connect <net_name> <container_name> //dynamically creates a NIC in a container on an existing virtual network
docker network disconnect <net_name> <container_name> //dynamically removes a NIC from a container

docker networks DNS
//DNS is the key to easy inter-container communication
//docker daemon has a built-in DNS server that containers use by default
docker container exec -it new_nginx ping nginx
//containers should not rely on IP addresses for communication
//DNS for friendly names is built-in if you use custom networks
//The default network bridge however does not have a DNS server by default, so you should use --link for communication
//Right, the default bridge network driver allow containers to communicate with each other when running on the same docker host.


image: image is an ordered collection of root filesystem changes and the corresponding execution parameters for 
use within a container  runtime.

Not a complete OS.No kernel,kernel modules.

docker tag <image_name> <tag_name>
//when you tag an existing image, you get the same image_id
docker history <image_name>:version

//official repositories live at the root namespace of the registry, They do not need account_name in front of
//<repo_name>

docker login
docker image push <image_name>
dcoker image tag <old_image_name> <new_image_name>
docker build -f <dockerfile> -t tag .
docker container rm <container_1>

volumes and persistence

docker volumes ls
docker volume inspect <volume_name>
docker container run -d --name maria -e MARIADB_ALLOW_EMPTY_ROOT_PASSWORD=true -v mariadb-vol:/var/lib/mariadb mariadb

persistent data: bind mounting

* maps a host file or directory to a container file or directory
* basically just two locations pointing to the same file
* Again, skips UFS and host files overwrite in any container

docker container run -d --name nginx -p 80:80 -v %cd%:/usr/share/nginx/html nginx
// this maps the pwd to the nginx container path
// any changes to the pwd, reflects in the container


Before making a new volume in a docker run -v command for a MySQL container, where could you look to see where MySQL expects the data path to be?

ans: Docker Hub


Assignment:

Database upgrade with containers

create a postgres container with named volume psql-data using versio 9.6.1
user docker hub to learn volume path and versions needed to run it.
check logs,stop container
create a new postgres container with same named volume using 9.6.2
check logs to validate


Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts:

Volumes are easier to back up or migrate than bind mounts.
You can manage volumes using Docker CLI commands or the Docker API.
Volumes work on both Linux and Windows containers.
Volumes can be more safely shared among multiple containers.
Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality.
New volumes can have their content pre-populated by a container.
Volumes on Docker Desktop have much higher performance than bind mounts from Mac and Windows hosts.

all the docker volumes are in the path /var/lib/volumes/<vol_name>/_data


docker_compose 

configure relationship between containers
save docker container run setings in a easy to read file
comprised of two separate but related things:

1. yaml formatted file that describes our solutions for- containers,networks,volumes
2. a cli tool called docker-compose for local dev/test automation tool

General format:

version: '3.1'
services: # same as docker run
    servicename: # a friendly name.This is also DNS name inside network
        image: # optional if you use build
        command: # optional. Replace the default CMD specified by the image
        environment: #optional. Same as -e in docker run
        volumes:
        ports:
        depends_on:
    servicename2:
        image:
        command:
        environment:
        volumes:
        ports:
        depends_on:
volumes: #Optional, same as docker create volume
networks: #Optional, same as docker create network

instead of list we use a -
docker compose is not a production grade tool. It is ideal for local development and test

docker compose up #sets up volumes/netwroks and start all containers
docker compose down #stop all containers and remove containers/volumes/net
docker compose ls

Version 2 and above provide significantly more features then the old default version 1,
and what we will be using as a default base for this course. Bonus Note: v2.x is actually
better for local docker-compose use, and v3.x is better for use in server clusters (Swarm and Kubernetes)

docker volumes revise:

Volumes are preferred mechanism for persisting data generated by and used by docker containers




docker swarm 

docker info
docker swarm init
docker swarm leave 


docker service create alpine ping 8.8.8.8
docker service ls
docker container ls
docker service ps <service_name>
docker service update <service_name> --replicas <number>

use docker playground 

create three nodes 

docker swarm init in node 1
copy and paste the tokens generated in node 1 to node 2 and node 3
docker node update --role manager node 2

docker node ls 
docker swarm join-token manager // returns token to join
watch docker service ls



In node 1:
docker service create --replicas 3 alpine ping 8.8.8.8

This will create 3 containers of alpine in node1,node2 and node 3

Docker Engine 1.12 introduces swarm mode that enables you to create a cluster of one or more Docker Engines called a swarm.
A swarm consists of one or more nodes: physical or virtual machines running Docker Engine 1.12 or later in swarm mode.

There are two types of nodes: managers and workers.

Manager nodes
Manager nodes handle cluster management tasks:

maintaining cluster state
scheduling services
serving swarm mode HTTP API endpoints
Using a Raft implementation, the managers maintain a consistent internal state of the entire swarm and all the services running on it. For testing purposes it is OK to run a swarm with a single manager. If the manager in a single-manager swarm fails, your services continue to run, but you need to create a new cluster to recover.

To take advantage of swarm mode’s fault-tolerance features, Docker recommends you implement an odd number of nodes according to your organization’s high-availability requirements. When you have multiple managers you can recover from the failure of a manager node without downtime.

A three-manager swarm tolerates a maximum loss of one manager.
A five-manager swarm tolerates a maximum simultaneous loss of two manager nodes.
An odd number N of manager nodes in the cluster tolerates the loss of at most (N-1)/2 managers. Docker recommends a maximum of seven manager nodes for a swarm.

Important Note: Adding more managers does NOT mean increased scalability or higher performance. In general, the opposite is true.

Worker nodes
Worker nodes are also instances of Docker Engine whose sole purpose is to execute containers. Worker nodes don’t participate in the Raft distributed state, make scheduling decisions, or serve the swarm mode HTTP API.

You can create a swarm of one manager node, but you cannot have a worker node without at least one manager node. By default, all managers are also workers. In a single manager node cluster, you can run commands like docker service create and the scheduler places all tasks on the local Engine.

To prevent the scheduler from placing tasks on a manager node in a multi-node swarm, set the availability for the manager node to Drain. The scheduler gracefully stops tasks on nodes in Drain mode and schedules the tasks on an Active node. The scheduler does not assign new tasks to nodes with Drain availability.

Refer to the docker node update command line reference to see how to change node availability.





kubernetes container orchestration

kubernetes: whole orchestration system.pronounced as kube-8s or kube for short.
kubectl: cli to configure kubernetes and manage apps. Pronounced as "kube control".
node: single server in kubernetes cluster
kubelet:kubernetes agent running on node
control panel: set of containers that manage the cluster. Includes api server,scheduler,controller manager,etcd
and more. Sometimes called master.


Kubernetes is a sort of meta-process that grants the ability to automate the deployment and scaling of several
containers at once. Several containers running the same application are grouped together. These containers act
as replicas and serve to load balance the incoming requests. A container orchestrator, then, supervises these groups
,ensuring they are operating correctly.


https://sensu.io/blog/how-kubernetes-works

A container orchestrator is essentially an administrator in charge of operating a fleet of containerized
applications. If a container needs to be restarted or acquire more resources, 
the orchestrator takes care of it for you.


A Kubernetes pod is a group of containers, and is the smallest unit that Kubernetes administers. 
Pods have a single IP address that is applied to every container within the pod. 
Containers in a pod share the same resources such as memory and storage. 
This allows the individual Linux containers inside a pod to be treated collectively as a single application,
as if all the containerized processes were running together on the same host in more traditional workloads.
It’s quite common to have a pod with only a single container, when the application or service is a single process
that needs to run. But when things get more complicated, and multiple processes need to work together using
the same shared data volumes for correct operation, multi-container pods ease deployment configuration compared
to setting up shared resources between containers on your own.


Deployments:

Kubernetes deployments define the scale at which you want to run your application by letting you set
the details of how you would like pods replicated on your Kubernetes nodes.
Deployments describe the number of desired identical pod replicas to run and the
preferred update strategy used when updating the deployment. 
Kubernetes will track pod health, and will remove or add pods as needed to bring your
application deployment to the desired state.


Services:

A service is an abstraction over the pods, and essentially, the only interface the various application consumers
interact with. As pods are replaced, their internal names and IPs might change.
A service exposes a single machine name or IP address mapped to pods whose underlying names and 
numbers are unreliable. A service ensures that, to the outside network, everything appears to be unchanged.
Network endpoint to conect to a pod


Nodes:

A Kubernetes node manages and runs pods; it’s the machine (whether virtualized or physical) that performs
the given work. Just as pods collect individual containers that operate together,
a node collects entire pods that function together. When you’re operating at scale, 
you want to be able to hand work over to a node whose pods are free to take it.


The Kubernetes control plane


The Kubernetes control plane is the main entry point for administrators and users to manage the various nodes. 
Operations are issued to it either through HTTP calls or connecting to the machine and
running command-line scripts. 
As the name implies, it controls how Kubernetes interacts with your applications.


Cluster

A cluster is all of the above components put together as a single unit.


Control plane

API Server
The API server exposes a REST interface to the Kubernetes cluster. 
All operations against pods, services, and so forth, are executed programmatically by communicating
with the endpoints provided by it.

Scheduler
The scheduler is responsible for assigning work to the various nodes.
It keeps watch over the resource capacity and ensures that a worker node’s 
performance is within an appropriate threshold.

Controller manager
The controller-manager is responsible for making sure that the shared state of the
cluster is operating as expected. More accurately, the controller manager oversees 
various controllers which respond to events (e.g., if a node goes down).

kubectl commands:

kubectl version
kubetcl version --short

kubectl run nginx-pod --image nginx
kubectl get pods
kubectl get all

unlike docker you cannot create containers directly in k8s.
you create a pod via cli,yaml or api. Kubernetes then creates containers inside it.
kubelet tells container runtime to create containers for you.
cli->control panel(etcd,api server,controller-manager,scheduler)->kubelet->container runtime->

kubectl create deployment nignx-deployment --image nginx
kubectl scale deploy <deployment_name> --replicas 2
kubectl scale deploy/<deployment_name> --replicas 2


kubectl logs deployment/my-apache
kubectl logs deployment/my-apache --follow //much like watch mode
kubectl logs deployment/my-apache --follow --tail 1 //give the last line of the update
kubectl describe pod <pod_name>
kubectl describe deployment <deployment_name>

kubectl get pods -w 



service types:

1. Custer IP:
single,internal virtual IP allocated
only reachable within the cluster (nodes and pods)
pods can reach service on apps port number


2. NodePort:

high port allocated on each node
port is open on every node's IP
anyone can connect (if they can reach node)
other pods needs to be updated on this port


Above service types are available in kubernetes infrastructure

3. LoadBalancer:

controls a LB endpoint external to the cluster
Available through a infra provider
Creates NodePort+ClusterIP services, tells LB to send to NodePort

4. ExternalName

Adds CName DNS record to CoreDNS 
Not used for pods, but for giving pods a DNS
name to use for something outisde kubernetes

//generator examples
kubectl create deployment test --image nginx --dry-run (dry runs but does not create a deployment)
kubectl create deployment test --image nginx --dry-run -o yaml (shows the yaml output)
kubectl create job test --image nginx --dry-run -o yaml (just creates a pod that runs ony once)



 

Three approaches to kubernetes management:

1. imperative commands: run,expose,scale,edit,create deployment
    Best for dev/learning/personal projects
2. Imperative objects: create -f file.yaml, replace -f file.yaml, delete 
3. Declarative objects: apply -f file.yaml or dir\, diff

kubectl apply -f myfile.yaml //create or update resources in a file
kubectl apply -f myyaml/ //create or update a whole document of yaml
kubectl apply -f https://bret.run/pod.yaml //create or update from a url 
