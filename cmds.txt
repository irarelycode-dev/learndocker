docker pull
docker run <image>
docker run -p host_port:container_port <image>
docker run -d -p host_port:container_port <image>
docker run --detach -p host_port:container_port <image>
docker stop <container_id/container_name>
docker run -d -p host_port:container_port --name <container_name> <image>
docker ps or docker container ls
docker ps -a or docker container ls -a
docker logs <container_name>
docker top <container_name> //displays the running process of the container

what happens when in "docker run"
1. Looks for image locally in the cache
2. If not, looks in the remote image repository (defaults to Docker Hub)
3. download the latest version (nginx:latest by default)
4. Creates a new container based on that image and prepares to start
5. Gives it a virtual IP on a private network inside a docker engine
6. opens up a port and runs

docker image rm <image_name>
docker exec -it <image_name> sh
docker start <container_id>
docker kill <container_id>
docker stop <container_id>
docker system prune //removes all containers, all networks not used by atleast one container, all dangling images and build cache
docker images
docker container rm <container_name>
docker container inspeact <container_id> //returns a json of how the container was started.
docker container stats //returns live status of the running containers
docker container run -it //starts new container interactively
docker container exec -it //run additional command in existing container

docker exec -it ubuntu sh
# apt-get install -y curl
#curl google.com
#exit 

docker exec -it mariadb bash //this runs an additional process on an existing running container
#ps aux  //it is used to monitor processes running on a linux system



docker networks
1. each container is connected to a private virtual network called "bridge"
2.Each virtual network routes through NAT firewall on host IP
3.All containers on a virtual network can talk to each other without -p
4.Best practice is to create a new virtual network for each app


docker network ls
docker network inspect <net_name> //summary of the network. All containers running on that network.
docker run -d --name new_nginx --network my_app_net nginx
docker network create my_app_net
docker network connect <net_name> <container_name> //dynamically creates a NIC in a container on an existing virtual network
docker network disconnect <net_name> <container_name> //dynamically removes a NIC from a container

docker networks DNS
//DNS is the key to easy inter-container communication
//docker daemon has a built-in DNS server that containers use by default
docker container exec -it new_nginx ping nginx
//containers should not rely on IP addresses for communication
//DNS for friendly names is built-in if you use custom networks
//The default network bridge however does not have a DNS server by default, so you should use --link for communication
//Right, the default bridge network driver allow containers to communicate with each other when running on the same docker host.


image: image is an ordered collection of root filesystem changes and the corresponding execution parameters for 
use within a container  runtime.

Not a complete OS.No kernel,kernel modules.

docker tag <image_name> <tag_name>
//when you tag an existing image, you get the same image_id
docker history <image_name>:version

//official repositories live at the root namespace of the registry, They do not need account_name in front of
//<repo_name>

docker login
docker image push <image_name>
dcoker image tag <old_image_name> <new_image_name>
docker build -f <dockerfile> -t tag .
docker container rm <container_1>

volumes and persistence

docker volumes ls
docker volume inspect <volume_name>
docker container run -d --name maria -e MARIADB_ALLOW_EMPTY_ROOT_PASSWORD=true -v mariadb-vol:/var/lib/mariadb mariadb

persistent data: bind mounting

* maps a host file or directory to a container file or directory
* basically just two locations pointing to the same file
* Again, skips UFS and host files overwrite in any container

docker container run -d --name nginx -p 80:80 -v %cd%:/usr/share/nginx/html nginx
// this maps the pwd to the nginx container path
// any changes to the pwd, reflects in the container


Before making a new volume in a docker run -v command for a MySQL container, where could you look to see where MySQL expects the data path to be?

ans: Docker Hub


Assignment:

Database upgrade with containers

create a postgres container with named volume psql-data using versio 9.6.1
user docker hub to learn volume path and versions needed to run it.
check logs,stop container
create a new postgres container with same named volume using 9.6.2
check logs to validate


Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts:

Volumes are easier to back up or migrate than bind mounts.
You can manage volumes using Docker CLI commands or the Docker API.
Volumes work on both Linux and Windows containers.
Volumes can be more safely shared among multiple containers.
Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality.
New volumes can have their content pre-populated by a container.
Volumes on Docker Desktop have much higher performance than bind mounts from Mac and Windows hosts.

all the docker volumes are in the path /var/lib/volumes/<vol_name>/_data


docker_compose 

configure relationship between containers
save docker container run setings in a easy to read file
comprised of two separate but related things:

1. yaml formatted file that describes our solutions for- containers,networks,volumes
2. a cli tool called docker-compose for local dev/test automation tool

General format:

version: '3.1'
services: # same as docker run
    servicename: # a friendly name.This is also DNS name inside network
        image: # optional if you use build
        command: # optional. Replace the default CMD specified by the image
        environment: #optional. Same as -e in docker run
        volumes:
        ports:
        depends_on:
    servicename2:
        image:
        command:
        environment:
        volumes:
        ports:
        depends_on:
volumes: #Optional, same as docker create volume
networks: #Optional, same as docker create network

instead of list we use a -
docker compose is not a production grade tool. It is ideal for local development and test

docker compose up #sets up volumes/netwroks and start all containers
docker compose down #stop all containers and remove containers/volumes/net
docker compose ls

Version 2 and above provide significantly more features then the old default version 1,
and what we will be using as a default base for this course. Bonus Note: v2.x is actually
better for local docker-compose use, and v3.x is better for use in server clusters (Swarm and Kubernetes)


docker swarm 

docker info
docker swarm init
docker swarm leave 


docker service create alpine ping 8.8.8.8
docker service ls
docker container ls
docker service ps <service_name>
docker service update <service_name> --replicas <number>

use docker playground 

create three nodes 

docker swarm init in node 1
copy and paste the tokens generated in node 1 to node 2 and node 3
docker node update --role manager node 2

docker node ls 
docker swarm join-token manager // returns token to join



In node 1:
docker service create --replicas 3 alpine ping 8.8.8.8

This will create 3 containers of alpine in node1,node2 and node 3

Docker Engine 1.12 introduces swarm mode that enables you to create a cluster of one or more Docker Engines called a swarm.
A swarm consists of one or more nodes: physical or virtual machines running Docker Engine 1.12 or later in swarm mode.

There are two types of nodes: managers and workers.

Manager nodes
Manager nodes handle cluster management tasks:

maintaining cluster state
scheduling services
serving swarm mode HTTP API endpoints
Using a Raft implementation, the managers maintain a consistent internal state of the entire swarm and all the services running on it. For testing purposes it is OK to run a swarm with a single manager. If the manager in a single-manager swarm fails, your services continue to run, but you need to create a new cluster to recover.

To take advantage of swarm mode’s fault-tolerance features, Docker recommends you implement an odd number of nodes according to your organization’s high-availability requirements. When you have multiple managers you can recover from the failure of a manager node without downtime.

A three-manager swarm tolerates a maximum loss of one manager.
A five-manager swarm tolerates a maximum simultaneous loss of two manager nodes.
An odd number N of manager nodes in the cluster tolerates the loss of at most (N-1)/2 managers. Docker recommends a maximum of seven manager nodes for a swarm.

Important Note: Adding more managers does NOT mean increased scalability or higher performance. In general, the opposite is true.

Worker nodes
Worker nodes are also instances of Docker Engine whose sole purpose is to execute containers. Worker nodes don’t participate in the Raft distributed state, make scheduling decisions, or serve the swarm mode HTTP API.

You can create a swarm of one manager node, but you cannot have a worker node without at least one manager node. By default, all managers are also workers. In a single manager node cluster, you can run commands like docker service create and the scheduler places all tasks on the local Engine.

To prevent the scheduler from placing tasks on a manager node in a multi-node swarm, set the availability for the manager node to Drain. The scheduler gracefully stops tasks on nodes in Drain mode and schedules the tasks on an Active node. The scheduler does not assign new tasks to nodes with Drain availability.

Refer to the docker node update command line reference to see how to change node availability.
